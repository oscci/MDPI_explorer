---
title: "MDPIexploreR - A guide"
---

## Background {.unnumbered}

Ever changing scientific publishing strategies shape academic communications.

To date, MDPI is the largest publisher of Open Access articles in the world and top-3 overall publisher (Right after Elsevier and SpringerNature). "[The Strain on Scientific Publishing](https://arxiv.org/abs/2309.15884)" highlights them as a frequent outlier for several metrics, but also as one of the most transparent major publishers out there.

This R package intends to help users to obtain factual data from MDPI's journals, directly from their website (via web-scraping) and nothing else. A compilation of several functions to explore articles metrics across all MDPI journals and special issues.

## Installing MDPIexploreR

```{r eval=FALSE}
devtools::install_github("pgomba/MDPI_explorer")
library(MDPIexploreR)
```

::: callout-note
Last update: December 2023
:::

## Data extracting functions {.unnumbered}

`article_find()`: The sole purpose of this function is to compile a vector with URLs for the papers of the target journal. Some journals might have more than one sitemap (sitemaps are limited to 50,000 urls), but `article_find()` is able to solve dual sitemaps. To use the function is only necessary to input a string with the code name of the target journal:

```{r eval=FALSE}

urls<-journal_papers("agriculture")

```

The journal code name usually coincides with the journal title, but this is not always the case if the journal name is too long. To find the code name for your journal of interest check the dataset `MDPI_journals`, included in the package:

```{r message=FALSE, echo=FALSE }
library(MDPIexploreR)
```

```{r}
MDPI_journals|>
  head(10)
```

`articles_info()`: Uses the vector produced by `journal_papers()` to, by web scraping each URL, obtain a data frame with information on editorial times (when article was received and accepted), turnaround times (time differential between acceptance and submission), data on type of issue where the article was published and type of article. The function allows to include a delay between web scraping instances (I recommend using 2 seconds to avoid having much impact in their servers) and, the possibility of only scraping a sample of all URLs

```{r eval=FALSE}

#Will scrape all url values leaving 2 seconds between iterations
data<-articles_info(urls,2) 

#Will scrape 1000 random papers from the url vector every 5 seconds
data<-articles_info(urls,2,1000)

```

::: callout-important
A stable internet connection is recommended, specially for web scraping large numbers of papers
:::

## Special issues & guest editors.

As per MDPI Special Issues policy, these "may publish contributions from the Guest Editor(s), but the number of such contributions should be limited to 25%". The function `guest_editor()` finds all closed special issues and calculate the proportion of articles with guest editors (`prop_flag` column) in then, including an option to delay special issues iterations. Additionally, calculates the difference between when the special issue was closed and the publication time of the last accepted article.

```{r eval=FALSE}
guest_editor("sustainability",2)

```

![](images/special_issue.PNG){fig-align="center"}

This function is inspired by [MA Oviedo-GarcÃ­a](https://twitter.com/maoviedogarcia) work on MDPI's special issues.

## Plotting functions

The plotting functions help to summarize the collected data. They only need a data.frame obtained via `articles_info()` and a string with the name of the journal. Some examples:

```{r eval=FALSE}

summary_graph(data,"Agriculture")

```

![](images/agriculture_summary_graph.png)

```{r eval=FALSE}

average_graph(data,"Agriculture")
```

![](images/agriculture_average_graph.png)

```{r eval=FALSE}
issues_graph(data,"Agriculture")

```

![](images/agriculture_issues_graph.png)

```{r eval=FALSE}

types_graph(data,"Agriculture")
```

![](images/agriculture_types_graph.png)

## Tips & Tricks

::: callout-tip
Web scraping large amounts of URLs can be time consuming (2 seconds per paper, depending on delay) and many things can go wrong during the process (problematic URLs, being kicked out of the server...). My advice is to split large URL vectors in smaller ones
:::
