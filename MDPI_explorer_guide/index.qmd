---
title: "Get started! A guide to MDPIexploreR"
---

<!-- badges: start -->
[![R-CMD-check](https://github.com/pgomba/MDPI_explorer/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/pgomba/MDPI_explorer/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

## Background {.unnumbered}

Ever changing scientific publishing strategies shape academic communications.

To date, MDPI is the largest publisher of Open Access articles in the world and top-3 overall publisher (Right after Elsevier and SpringerNature). "[The Strain on Scientific Publishing](https://arxiv.org/abs/2309.15884)" highlights them as a frequent outlier for several metrics, but also as one of the most transparent major publishers out there.

This R package intends to help users to obtain factual data from MDPI's journals, special issues and articles directly from their website (via web-scraping). Detailed information on functions and datasets can be found in the [Reference](reference.html) section. 

The following section aims to provide a brief and approachable tutorial introducing users to the functionalities of the R package MDPIexploreR. 


## Installing MDPIexploreR

```{r eval=FALSE}
devtools::install_github("pgomba/MDPI_explorer")
library(MDPIexploreR)
```


## Data collecting functions

### article_find() {.unnumbered}

The sole purpose of `article_find()` is to output a vector with URLs from all available papers from the target journal. To do so, the function navigate the sitemap/s containing journal links and filter out those not belonging to scientific articles. The function requires as input the code of the target journal.

```{r eval=FALSE}

urls<-journal_papers("agriculture")

```

The journal code name usually coincides with the journal title, but this is not always the case if the journal name is too long. To find the code name for your journal of interest check the dataset `MDPI_journals`, included in the package:

```{r message=FALSE, echo=FALSE }
library(MDPIexploreR)
```

```{r}
MDPI_journals|>
  head(10)
```

### articles_info()

Banking on the results from `article_find()` or any vector with scientific papers URLs, `article_info()` web scrapes each URL to compile a data frame with information on each paper editorial times (when article was received and accepted), turnaround times (time differential between acceptance and submission), data on type of issue where the article was published and type of article. The function allows to include a delay between web scraping instances (2 seconds, by default) and, the possibility of only scraping a sample of all URLs.

```{r eval=FALSE}

#Will scrape all url values leaving 2 seconds between iterations
data<-articles_info(urls) 

#Will scrape 1000 random papers from the url vector every 1 seconds
data<-articles_info(urls,1,1000)

```

::: callout-important
A stable internet connection is recommended, specially for web scraping large numbers of papers
:::

### special_issue_find()

This function compiles all special issues available in the target journal and returns a vector with URLs. By default, finds only CLOSED special issues, but this behaviour can be changed by altering the type parameter to "open" or "all".

```{r eval=FALSE}

# Creates a vector with all CLOSED special issues from the journal Covid
URLs<-special_issue_find("covid")

# Creates a vector with all special issues from the journal Covid
URLs<-special_issue_find("covid", type="all")

```

### guest_editor_info()

Using a vector with special issues URLs, `guest_editor_info()` outputs a data frame containing information on number of non-editorial papers, the amount of papers on which guest editors have participated and a comparison between special issue closing date and when was the last article really submitted.

```{r eval=FALSE}

URLs<-special_issue_find("covid")

# Extract data from all URLs, iterating every 3 seconds
guest_editor_info (URLs, sleep=3)

# Extract data from 2 URLs, iterating every 2 seconds (default)
guest_editor_info (URLs, sample_size=2)

```

This function is inspired by [MA Oviedo-GarcÃ­a](https://twitter.com/maoviedogarcia) work on MDPI's special issues.

## Plotting functions

The plotting functions help to summarize the collected data. They only need a data.frame obtained via `articles_info()` and a string with the name of the journal. Some examples:

```{r eval=FALSE}

summary_graph(data,"Agriculture")

```

![](images/agriculture_summary_graph.png)

```{r eval=FALSE}

average_graph(data,"Agriculture")
```

![](images/agriculture_average_graph.png)

```{r eval=FALSE}
issues_graph(data,"Agriculture")

```

![](images/agriculture_issues_graph.png)

```{r eval=FALSE}

types_graph(data,"Agriculture")
```

![](images/agriculture_types_graph.png)

## Tips & Tricks

::: callout-tip
Web scraping large amounts of URLs can be time consuming (2 seconds per paper, depending on delay) and many things can go wrong during the process (problematic URLs, being kicked out of the server...). My advice is to split large URL vectors in smaller ones
:::
