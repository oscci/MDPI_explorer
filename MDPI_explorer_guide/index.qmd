---
title: "MDPIexploreR - A guide"
---

## Background {.unnumbered}

Inspired by 2021 [Paolo Crosetto](https://twitter.com/PaoloCrosetto)'s blog ([Is MDPI a predatory publisher?](https://paolocrosetto.wordpress.com/2021/04/12/is-mdpi-a-predatory-publisher/)), I wanted to see for myself some of MDPIÂ´s journal metrics, obtain factual data and, help others to do the same. A lot has happened since this repository/guide was published, including an awesome collaboration with [Mark Hanson](https://fediscience.org/@MarkHanson), [Paolo Crosetto](https://twitter.com/PaoloCrosetto) and [Dan Brockington](https://twitter.com/danbrockington) exploring the current Strain on Scientific Publishing \[[Link to project](https://the-strain-on-scientific-publishing.github.io/website/)\] \[[Link to preprint](https://arxiv.org/abs/2309.15884)\]. Initially, the repository was a collection of scripts to explore MDPI journals. Now, is been transformed to a full R package, with 2 functions to obtain data, 4 to plot this data and a dataset to play with

## Installing MDPIExploreR

```{r eval=FALSE}
devtools::install_github("pgomba/MDPI_explorer")
```

::: callout-note
Last update: December 2023
:::

## Data extracting functions {.unnumbered}

`journal_papers()`: This function looks into the target journal sitemap and returns a vector containing all journal papers urls. To use it, it only requires a string with the name of the target journal:

```{r eval=FALSE}

urls<-journal_papers("agriculture")

```

`articles_info()`: Uses the vector, or a sample from the vector obtained via `journal_papers()` to web scrap article data, outputting a data frame. The function requires the user to enter a delay (in seconds) between scraping instances. This is useful to not get blocked by the scraped server (suggested time = 2 seconds). Additionally, it is possible to scrap a random sample of the total of papers.

```{r eval=FALSE}

#Will scrape all url values leaving 2 seconds between iterations
data<-articles_info(urls,2) 

#Will scrape 1000 random papers from the url vector every 5 seconds
data<-articles_info(urls,2,1000)

```

## Plotting functions

The plotting functions help to summarize the collected data. They only need a data.frame obtained via `articles_info()` and a string with the name of the journal. Some examples:

```{r eval=FALSE}

summary_graph(data,"Agriculture")

```

![](images/agriculture_summary_graph.png)

```{r eval=FALSE}

average_graph(data,"Agriculture")
```

![](images/agriculture_average_graph.png)

```{r eval=FALSE}
issues_graph(data,"Agriculture")

```

![](images/agriculture_issues_graph.png)

```{r eval=FALSE}

types_graph(data,"Agriculture")
```

![](images/agriculture_types_graph.png)
