[["index.html", "Web scrapping MDPI publications with R The code Graphs", " Web scrapping MDPI publications with R Inspired by 2021 Paolo Crosetto’s blog (Is MDPI a predatory publisher?), I wanted to have a look at some MDPI journal metrics using web scraping tools (rvest) via R, presenting factual data without judging if they are or not of predatory nature (I’ll leave that to the reader), enabling others to use the same code to explore metrics from other MDPI journals. The code and some journal data sets are available here (GitHub link). Originally, data and code were shared via Twitter, and you can follow the original discussion here (Twitter link). The code The first step is to download the links from the journal web sitemap (thanks to twitter user @JorritGosens for helping make this part easier than I had planned). To do so, select a journal (use non-capital letters) and proceed to read the page and extract all the links available library(tidyverse) library(rvest) journal&lt;-&quot;sustainability&quot; sitemap&lt;-read_html(paste0(&quot;https://www.mdpi.com/sitemap/sitemap.&quot;,journal,&quot;.xml&quot;)) papers&lt;-sitemap%&gt;% html_nodes(&quot;loc&quot;)%&gt;% html_text2() These links contain more than the papers we are looking for. Using some keywords we can get rid of those links we are not interested on (announcements, news, blogs, etc). Keep in mind that a sitemap can’t contain more than 50,000 links, and therefore, if the journal has more than 50,000 papers these will continue in a different web sitemap. But, 50.000 is enough for what we plan to do here. cleaner&lt;- &quot;guide|even|topi|soci|subm|conf|section|issue|about|announcements|awa|indexing|instructions|apc|history|detailed_instructions|edit|imprint|toc-alert|stats|most_cited&quot; clean_papers&lt;-papers[-grep(cleaner, papers)] And that’s it. With some minor exceptions, clean_papers is now a list with url links to the papers. I’m going to focus now in extracting information from these papers, targeting editorial data (submission, revision, acceptance and publication time) along with whether or not they are part of a special issue. With a loop, I make rvest go, paper by paper, extracting this information, appending it to pubhistory (a list). Requesting a high number of papers in little time might end up with MDPI kicking you out of their servers. The function Sys.sleep(1) its there to slow down the speed of the loop by stopping 1 second everytime the loop starts again. I’ve had some success reducing this number on journals with less than 8,000 publications, and I think it needs further increase to tackle journals over 50,000 publications (working on it atm). This is because when MDPI detects a high volume of requests, the server kicks you out. If you have this problem, consider breaking clean_papers into smaller lists, and pass them one by one. pubhistory&lt;-list() for (i in clean_papers) { Sys.sleep(1) paper&lt;-read_html(i) ex_paper&lt;-paper%&gt;% html_nodes(&quot;.pubhistory&quot;)%&gt;% html_text2() ex_paper2&lt;-paper%&gt;% html_nodes(&quot;.belongsTo&quot;)%&gt;% html_text2() w&lt;-paste(i,&quot;-&quot;,ex_paper,&quot;-&quot;,ex_paper2) pubhistory&lt;-append(w,pubhistory) } The final step is to compile the table. You will notice some papers don’t have revision time. This might be because they were accepted straight away (no revisions needed). I have removed these from the final table (drop_na()) for simplicity in the analysis, but if you want to have all publications, remove the drop_na(). pub_table&lt;-do.call(rbind, pubhistory)%&gt;% as_tibble()%&gt;% separate(V1,sep=&quot; - &quot;,c(&quot;link&quot;,&quot;Publication&quot;,&quot;Special_issue&quot;))%&gt;% separate(Publication,sep=&quot;/&quot;,c(&quot;Received&quot;,&quot;Revised&quot;,&quot;Accepted&quot;,&quot;Published&quot;))%&gt;% drop_na()%&gt;% #remove papers accepted straight away mutate(Received= gsub(&quot;Received: &quot;,&quot;&quot;,Received))%&gt;% mutate(Received= lubridate::dmy(gsub(&quot; &quot;,&quot;/&quot;,Received)))%&gt;% mutate(Revised=gsub(&quot;Revised: &quot;,&quot;&quot;,Revised))%&gt;% mutate(Revised= lubridate::dmy(gsub(&quot; &quot;,&quot;/&quot;,Revised)))%&gt;% mutate(Accepted=gsub(&quot;Accepted: &quot;,&quot;&quot;,Accepted))%&gt;% mutate(Accepted= lubridate::dmy(gsub(&quot; &quot;,&quot;/&quot;,Accepted)))%&gt;% mutate(Published=gsub(&quot;Published: &quot;,&quot;&quot;,Published))%&gt;% mutate(Published= lubridate::dmy(gsub(&quot; &quot;,&quot;/&quot;,Published)))%&gt;% mutate(days=Published-Received)%&gt;% mutate(is_s_issue=if_else(Special_issue==&quot;&quot;,&quot;No&quot;,&quot;Yes&quot;)) Graphs The code for the graphs below is available in the GitHub repository linked at the beginning, and are just an example of what can be done with the data, but much more can be done (most published author, countries and institutions with more publications, abstract wordclouds, etc). Some key aspects to note are the reduction and homogenization of publication times, the increase in publications in recent years and the reliance in special issues. This has been noted before (see Is MDPI a predatory publisher? blog). MDPI Plants MDPI Diversity MDPI Microorganisms "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
