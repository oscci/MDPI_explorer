[
  {
    "objectID": "changelog.html",
    "href": "changelog.html",
    "title": "Changelog",
    "section": "",
    "text": "guest_editor() no longer part of MDPIexploreR.\nClered errors and warnings for R CMD check"
  },
  {
    "objectID": "changelog.html#v.-0.0.1.3-10-jan-23",
    "href": "changelog.html#v.-0.0.1.3-10-jan-23",
    "title": "Changelog",
    "section": "",
    "text": "guest_editor() no longer part of MDPIexploreR.\nClered errors and warnings for R CMD check"
  },
  {
    "objectID": "changelog.html#v.-0.0.1.2-6-jan-23",
    "href": "changelog.html#v.-0.0.1.2-6-jan-23",
    "title": "Changelog",
    "section": "v. 0.0.1.2 (6-Jan-23)",
    "text": "v. 0.0.1.2 (6-Jan-23)\nAdding functions special_issue_find() & guest_editor_info() to deprecate guest_editor() and make the workflow more flexible."
  },
  {
    "objectID": "changelog.html#v.-0.0.1.1-4-jan-23",
    "href": "changelog.html#v.-0.0.1.1-4-jan-23",
    "title": "Changelog",
    "section": "v. 0.0.1.1 (4-Jan-23)",
    "text": "v. 0.0.1.1 (4-Jan-23)\nguest_editor() now selects two first words of guest editors and authors names for comparison to account for discrepancies in number of surnames between guest editors and paper authors. (e.g. guest editor name = Dr. Perico Palotes Fernández, but author name is just Perico Palotes. ).\nguest_editor() d_over_deadline output column now considers latest article submission date to a special issue instead of latest acceptance time.\nguest_editor() Assoc. Prof. guest editors are now picked by the function too\nguest_editor() Ignores papers of type “Editorial”"
  },
  {
    "objectID": "changelog.html#v.-0.0.0.1-initial",
    "href": "changelog.html#v.-0.0.0.1-initial",
    "title": "Changelog",
    "section": "v. 0.0.0.1 (Initial)",
    "text": "v. 0.0.0.1 (Initial)"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Get started! A guide to MDPIexploreR",
    "section": "Background",
    "text": "Background\nEver changing scientific publishing strategies shape academic communications.\nTo date, MDPI is the largest publisher of Open Access articles in the world and top-3 overall publisher (Right after Elsevier and SpringerNature). “The Strain on Scientific Publishing” highlights them as a frequent outlier for several metrics, but also as one of the most transparent major publishers out there.\nThis R package intends to help users to obtain factual data from MDPI’s journals, special issues and articles directly from their website (via web-scraping). Detailed information on functions and datasets can be found in the Reference section.\nThe following section aims to provide a brief and approachable tutorial introducing users to the functionalities of the R package MDPIexploreR."
  },
  {
    "objectID": "index.html#installing-mdpiexplorer",
    "href": "index.html#installing-mdpiexplorer",
    "title": "Get started! A guide to MDPIexploreR",
    "section": "Installing MDPIexploreR",
    "text": "Installing MDPIexploreR\n\ndevtools::install_github(\"pgomba/MDPI_explorer\")\nlibrary(MDPIexploreR)"
  },
  {
    "objectID": "index.html#data-collecting-functions",
    "href": "index.html#data-collecting-functions",
    "title": "Get started! A guide to MDPIexploreR",
    "section": "Data collecting functions",
    "text": "Data collecting functions\n\narticle_find()\nThe sole purpose of article_find() is to output a vector with URLs from all available papers from the target journal. To do so, the function navigate the sitemap/s containing journal links and filter out those not belonging to scientific articles. The function requires as input the code of the target journal.\n\nurls&lt;-journal_papers(\"agriculture\")\n\nThe journal code name usually coincides with the journal title, but this is not always the case if the journal name is too long. To find the code name for your journal of interest check the dataset MDPI_journals, included in the package:\n\nMDPI_journals|&gt;\n  head(10)\n\n                               name            code\n1                         Acoustics       acoustics\n2                         Actuators       actuators\n3           Administrative Sciences          admsci\n4                       Adolescents     adolescents\n5  Advances in Respiratory Medicine             arm\n6                       Aerobiology     aerobiology\n7                         Aerospace       aerospace\n8                       Agriculture     agriculture\n9                   AgriEngineering agriengineering\n10                    Agrochemicals   agrochemicals\n\n\n\n\narticles_info()\nBanking on the results from article_find() or any vector with scientific papers URLs, article_info() web scrapes each URL to compile a data frame with information on each paper editorial times (when article was received and accepted), turnaround times (time differential between acceptance and submission), data on type of issue where the article was published and type of article. The function allows to include a delay between web scraping instances (2 seconds, by default) and, the possibility of only scraping a sample of all URLs.\n\n#Will scrape all url values leaving 2 seconds between iterations\ndata&lt;-articles_info(urls) \n\n#Will scrape 1000 random papers from the url vector every 1 seconds\ndata&lt;-articles_info(urls,1,1000)\n\n\n\n\n\n\n\nImportant\n\n\n\nA stable internet connection is recommended, specially for web scraping large numbers of papers\n\n\n\n\nspecial_issue_find()\nThis function compiles all special issues available in the target journal and returns a vector with URLs. By default, finds only CLOSED special issues, but this behaviour can be changed by altering the type parameter to “open” or “all”.\n\n# Creates a vector with all CLOSED special issues from the journal Covid\nURLs&lt;-special_issue_find(\"covid\")\n\n# Creates a vector with all special issues from the journal Covid\nURLs&lt;-special_issue_find(\"covid\", type=\"all\")\n\n\n\nguest_editor_info()\nUsing a vector with special issues URLs, guest_editor_info() outputs a data frame containing information on number of non-editorial papers, the amount of papers on which guest editors have participated and a comparison between special issue closing date and when was the last article really submitted.\n\nURLs&lt;-special_issue_find(\"covid\")\n\n# Extract data from all URLs, iterating every 3 seconds\nguest_editor_info (URLs, sleep=3)\n\n# Extract data from 2 URLs, iterating every 2 seconds (default)\nguest_editor_info (URLs, sample_size=2)\n\nThis function is inspired by MA Oviedo-García work on MDPI’s special issues."
  },
  {
    "objectID": "index.html#plotting-functions",
    "href": "index.html#plotting-functions",
    "title": "Get started! A guide to MDPIexploreR",
    "section": "Plotting functions",
    "text": "Plotting functions\nThe plotting functions help to summarize the collected data. They only need a data.frame obtained via articles_info() and a string with the name of the journal. Some examples:\n\nsummary_graph(data,\"Agriculture\")\n\n\n\naverage_graph(data,\"Agriculture\")\n\n\n\nissues_graph(data,\"Agriculture\")\n\n\n\ntypes_graph(data,\"Agriculture\")"
  },
  {
    "objectID": "index.html#tips-tricks",
    "href": "index.html#tips-tricks",
    "title": "Get started! A guide to MDPIexploreR",
    "section": "Tips & Tricks",
    "text": "Tips & Tricks\n\n\n\n\n\n\nTip\n\n\n\nWeb scraping large amounts of URLs can be time consuming (2 seconds per paper, depending on delay) and many things can go wrong during the process (problematic URLs, being kicked out of the server…). My advice is to split large URL vectors in smaller ones"
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "Reference (wip)",
    "section": "",
    "text": "article_find(): Returns a vector containing URLs for all scientific articles in a journal. Input: A text string with journal title or code. See dataset MDPI_journals for reference.\narticle_info(): Returns a data frame with editorial times, type and presence in special issue for each submitted article. Input: A vector with articles URLS. see article_find(). Optional: sleep (Number of seconds between scraping iterations. Default is 2 seconds) and sample_size (Size of the sample of articles to take from the vector. Leave blank to select all).\naverage_graph()\nguest_editor_info()\nissues_graph()\nspecial_issue_find()\nsummary_graph()\ntype_graph()\n\n\nagriculture\nhorticulturae\nMDPI_journals"
  },
  {
    "objectID": "reference.html#functions",
    "href": "reference.html#functions",
    "title": "Reference (wip)",
    "section": "",
    "text": "article_find(): Returns a vector containing URLs for all scientific articles in a journal. Input: A text string with journal title or code. See dataset MDPI_journals for reference.\narticle_info(): Returns a data frame with editorial times, type and presence in special issue for each submitted article. Input: A vector with articles URLS. see article_find(). Optional: sleep (Number of seconds between scraping iterations. Default is 2 seconds) and sample_size (Size of the sample of articles to take from the vector. Leave blank to select all).\naverage_graph()\nguest_editor_info()\nissues_graph()\nspecial_issue_find()\nsummary_graph()\ntype_graph()\n\n\nagriculture\nhorticulturae\nMDPI_journals"
  }
]